{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (set_seed,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          BertConfig,\n",
    "                          BertTokenizer,\n",
    "                          BertForSequenceClassification)\n",
    "from helper import b_metrics\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "seed = 1520\n",
    "set_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu00608/miniconda3/envs/ml/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "label2tag = {\n",
    "    0: \"news_story\",\n",
    "    1: \"news_culture\",\n",
    "    2: \"news_entertainment\",\n",
    "    3: \"news_sports\",\n",
    "    4: \"news_finance\",\n",
    "    5: \"news_house\",\n",
    "    6: \"news_car\",\n",
    "    7: \"news_edu\",\n",
    "    8: \"news_tech\",\n",
    "    9: \"news_military\",\n",
    "    10: \"news_travel\",\n",
    "    11: \"news_world\",\n",
    "    12: \"stock\",\n",
    "    13: \"news_agriculture\",\n",
    "    14: \"news_game\",\n",
    "}\n",
    "tag2label = {v: k for k, v in label2tag.items()}\n",
    "\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Average 30, \n",
    "# n_words = [len(i) for i in texts]\n",
    "# avg_words = sum(n_words)/len(n_words)\n",
    "# print(f\"Average words = {avg_words}\")\n",
    "max_length = 25\n",
    "\n",
    "n_labels = len(tag2label)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu00608/miniconda3/envs/ml/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/home/stu00608/miniconda3/envs/ml/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1678: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/stu00608/Code/gpt2-chinese/roberta-news-classification/train.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcilab/home/stu00608/Code/gpt2-chinese/roberta-news-classification/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mmodel/vocab.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcilab/home/stu00608/Code/gpt2-chinese/roberta-news-classification/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m config \u001b[39m=\u001b[39m BertConfig\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mmodel/config.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcilab/home/stu00608/Code/gpt2-chinese/roberta-news-classification/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mmodel/pytorch_model.bin\u001b[39;49m\u001b[39m'\u001b[39;49m, config\u001b[39m=\u001b[39;49mconfig, num_labels\u001b[39m=\u001b[39;49mn_labels)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcilab/home/stu00608/Code/gpt2-chinese/roberta-news-classification/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/transformers/modeling_utils.py:2113\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2112\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2113\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2115\u001b[0m \u001b[39mif\u001b[39;00m device_map \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2116\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39m_no_split_modules \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_labels'"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('model/vocab.txt')\n",
    "config = BertConfig.from_pretrained('model/config.json')\n",
    "model = BertForSequenceClassification.from_pretrained('model/pytorch_model.bin', config=config, num_labels=n_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_text, max_length, tokenizer):\n",
    "    '''\n",
    "    Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "        - input_ids: list of token ids\n",
    "        - token_type_ids: list of token type ids\n",
    "        - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "    '''\n",
    "    return tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        add_special_tokens = True,\n",
    "        max_length = max_length,\n",
    "        # truncation = True,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/toutiao_cat_data.txt\"\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "token_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "with open(path) as f:\n",
    "    for line in f.readlines():\n",
    "        split = line.split(\"_!_\")\n",
    "        texts.append(split[3])\n",
    "        labels.append(int(split[1]))\n",
    "    \n",
    "    texts = texts[:10000]\n",
    "    labels = labels[:10000]\n",
    "\n",
    "for sample in tqdm(texts):\n",
    "    encoding_dict = preprocessing(sample, max_length, tokenizer)\n",
    "    token_ids.append(encoding_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoding_dict[\"attention_mask\"])\n",
    "\n",
    "token_ids = torch.cat(token_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in token_ids[:10]:\n",
    "    print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average words in a sentence.\n",
    "n_words = [len(i) for i in texts]\n",
    "n_words = np.array(n_words)\n",
    "print(np.mean(n_words), np.std(n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "epochs = 5\n",
    "\n",
    "# Indices of the train and validation splits stratified by labels\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = val_ratio,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_ids[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_ids[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, dataloader, optimizer, scheduler, device):\n",
    "    # Take global model.\n",
    "    global model\n",
    "\n",
    "    # Tracking variables.\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    # Total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(dataloader, total=len(dataloader), unit=\"batch\") as tepoch:\n",
    "        tepoch.set_description(f\"Epoch {epoch}/{epochs}\")\n",
    "        for batch in tepoch:\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            # TODO: Check what's the return structure looks like.\n",
    "            train_output = model(\n",
    "                b_input_ids, \n",
    "                token_type_ids = None, \n",
    "                attention_mask = b_input_mask, \n",
    "                labels = b_labels)\n",
    "            # Backward pass\n",
    "            train_output.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # Update tracking variables\n",
    "            loss = train_output.loss.item()\n",
    "            total_loss += loss\n",
    "\n",
    "            tepoch.set_postfix(loss=loss)\n",
    "    \n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "    return avg_epoch_loss\n",
    "\n",
    "def validation(dataloader, device):\n",
    "    global model\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "    #total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluatdion.\n",
    "    model.eval()\n",
    "\n",
    "    with tqdm(dataloader, total=len(dataloader), unit=\"batch\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                eval_output = model(\n",
    "                    b_input_ids, \n",
    "                    token_type_ids = None, \n",
    "                    attention_mask = b_input_mask, \n",
    "                )\n",
    "            logits = eval_output.logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            # Calculate validation metrics\n",
    "            b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "            val_accuracy.append(b_accuracy)\n",
    "            # Update precision only when (tp + fp) !=0; ignore nan\n",
    "            if b_precision != 'nan': val_precision.append(b_precision)\n",
    "            # Update recall only when (tp + fn) !=0; ignore nan\n",
    "            if b_recall != 'nan': val_recall.append(b_recall)\n",
    "            # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "            if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "\n",
    "    \n",
    "    return val_accuracy, val_precision, val_recall, val_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n",
    "    eps = 1e-8 # default is 1e-8.\n",
    ")\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "all_loss = {\n",
    "    \"train_loss\": [],\n",
    "    \"valid_loss\": []\n",
    "}\n",
    "all_acc = {\n",
    "    \"train_acc\": [],\n",
    "    \"valid_acc\": []\n",
    "}\n",
    "all_precision = {\n",
    "    \"train_precision\": [],\n",
    "    \"valid_precision\": []\n",
    "}\n",
    "all_recall = {\n",
    "    \"train_recall\": [],\n",
    "    \"valid_recall\": []\n",
    "}\n",
    "all_specificity = {\n",
    "    \"train_specificity\": [],\n",
    "    \"valid_specificity\": []\n",
    "}\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss = train(epoch, train_dataloader, optimizer, scheduler, device)\n",
    "\n",
    "    val_accuracy, val_precision, val_recall, val_specificity = validation(\n",
    "        validation_dataloader,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    epoch_loss = train_loss / epoch*len(train_dataloader)\n",
    "    epoch_acc = sum(val_accuracy)/len(val_accuracy)\n",
    "    epoch_precision = sum(val_precision)/len(val_precision)\n",
    "    epoch_recall = sum(val_recall)/len(val_recall)\n",
    "    epoch_specificity = sum(val_specificity)/len(val_specificity)\n",
    "\n",
    "    all_loss['train_loss'].append(epoch_loss) \n",
    "    all_acc['valid_acc'].append(epoch_acc)\n",
    "    all_precision['valid_precision'].append(epoch_precision)\n",
    "    all_recall['valid_recall'].append(epoch_recall)\n",
    "    all_specificity['valid_specificity'].append(epoch_specificity)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(epoch_loss))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(epoch_acc))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(epoch_precision) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(epoch_recall) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(epoch_specificity) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n",
    "\n",
    "\n",
    "plot_dict(all_loss, use_xlabel=\"Epochs\", use_ylabel=\"Value\", use_linestyles=['-', '--'])\n",
    "plot_dict(all_acc, use_xlabel=\"Epochs\", use_ylabel=\"Value\", use_linestyles=['-', '--'])\n",
    "plot_dict(all_precision, use_xlabel=\"Epochs\", use_ylabel=\"Value\", use_linestyles=['-', '--'])\n",
    "plot_dict(all_recall, use_xlabel=\"Epochs\", use_ylabel=\"Value\", use_linestyles=['-', '--'])\n",
    "plot_dict(all_specificity, use_xlabel=\"Epochs\", use_ylabel=\"Value\", use_linestyles=['-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49df5ff4a6ba4bb9d2ddca8074f5634ee6d89cf9b3a0514151aa539bb156f8f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
